#!/bin/bash
#############################################################################
# Process complete LangAge16kHz dataset with GPU acceleration              #
# Description: Generate CSV with all segments from the full dataset        #
# Author: Hanno MÃ¼ller                                                      #
# Date: 2025-09-03                                                          #
#############################################################################

echo "Processing Complete LangAge16kHz Dataset (GPU Accelerated)"
echo "=========================================================="

# Check SLURM allocation
echo "SLURM Resource Allocation:"
echo "  Job ID: $SLURM_JOB_ID"
echo "  CPUs allocated: $SLURM_CPUS_PER_TASK"
echo "  GPUs allocated: $SLURM_GPUS_PER_TASK"
echo "  Nodes: $SLURM_JOB_NODELIST"
echo ""

# Activate virtual environment
source .venv/bin/activate

# Set paths for complete dataset
INPUT_DIR="data/LangAge16kHz"
OUTPUT_FILE="data/LangAgeTranscriptions.csv"
FINE_TUNED_MODEL="FrisperWhisper/largeV3"
CHECKPOINT="${1:-checkpoint-6000}"  # Default to checkpoint-6000

# GPU Configuration for full dataset
GPUS=4  # Use all 4 available GPUs
CPUS=40  # Use more CPUs for the full dataset
BATCH_SIZE=16  # Larger batch size for efficiency
TRANSCRIPTION_PROCESSES=1  # Single process to avoid conflicts

echo "Input directory: $INPUT_DIR"
echo "Output file: $OUTPUT_FILE"
echo "Fine-tuned model: $FINE_TUNED_MODEL"
echo "Checkpoint: $CHECKPOINT"
echo "Configuration: $GPUS GPUs, $CPUS CPUs, batch size $BATCH_SIZE"
echo "Transcription processes: $TRANSCRIPTION_PROCESSES"
echo ""

# Count total files
TOTAL_WAV_FILES=$(ls $INPUT_DIR/*.wav 2>/dev/null | wc -l)
TOTAL_TEXTGRID_FILES=$(ls $INPUT_DIR/*.TextGrid 2>/dev/null | wc -l)
echo "Dataset Overview:"
echo "  WAV files: $TOTAL_WAV_FILES"
echo "  TextGrid files: $TOTAL_TEXTGRID_FILES"
echo ""

# Check if GPUs are available
if command -v nvidia-smi >/dev/null 2>&1; then
    echo "GPU Status:"
    nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu --format=csv,noheader,nounits
    echo ""
fi

# Estimate processing time
echo "â±ï¸  Estimated processing time: 2-3 hours for complete dataset"
echo ""

# Run the complete pipeline with GPU acceleration
echo "ğŸš€ Starting complete dataset processing..."
START_TIME=$(date +%s)

python scripts/run_whisper_comparison_pipeline.py \
    --input "$INPUT_DIR" \
    --output "$OUTPUT_FILE" \
    --fine_tuned_model "$FINE_TUNED_MODEL" \
    --checkpoint "$CHECKPOINT" \
    --cpus "$CPUS" \
    --gpus "$GPUS" \
    --batch_size "$BATCH_SIZE" \
    --transcription_batch_processes "$TRANSCRIPTION_PROCESSES" \
    --steps all

END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))

echo ""
echo "ğŸ“Š Processing Results:"
echo "=============================="

if [ -f "$OUTPUT_FILE" ]; then
    echo "âœ… Output file created: $OUTPUT_FILE"
    
    # Count rows (excluding header)
    TOTAL_ROWS=$(tail -n +2 "$OUTPUT_FILE" | wc -l)
    echo "ğŸ“Š Total segments processed: $TOTAL_ROWS"
    
    # File size
    FILE_SIZE=$(ls -lh "$OUTPUT_FILE" | awk '{print $5}')
    echo "ğŸ“ File size: $FILE_SIZE"
    
    # Show processing time
    echo "â±ï¸  Total processing time: ${DURATION} seconds ($(($DURATION / 60)) minutes)"
    echo "âš¡ Average: $(echo "scale=2; $TOTAL_ROWS / ($DURATION / 60)" | bc) segments/minute"
    
    # Show column headers
    echo ""
    echo "ğŸ“‹ CSV Structure:"
    head -1 "$OUTPUT_FILE" | tr ',' '\n' | nl
    
    echo ""
    echo "ğŸ“„ Sample data (first 3 rows):"
    head -4 "$OUTPUT_FILE" | cut -c1-120
    
    echo ""
    echo "ğŸ¯ Quick Statistics:"
    if [ $TOTAL_ROWS -gt 0 ]; then
        # Count unique files processed
        UNIQUE_FILES=$(tail -n +2 "$OUTPUT_FILE" | cut -d',' -f1 | sort | uniq | wc -l)
        echo "   Unique files processed: $UNIQUE_FILES / $TOTAL_WAV_FILES"
        
        # Average segment duration
        AVG_DURATION=$(tail -n +2 "$OUTPUT_FILE" | cut -d',' -f10 | awk '{sum+=$1; count++} END {if(count>0) printf "%.2f", sum/count; else print "N/A"}')
        echo "   Average segment duration: $AVG_DURATION seconds"
        
        # Speaker distribution
        echo "   Speaker distribution:"
        tail -n +2 "$OUTPUT_FILE" | cut -d',' -f5 | sort | uniq -c | head -5
    fi
    
    # Show intermediate files
    echo ""
    echo "ğŸ“ Intermediate files:"
    if [ -d "data/LangAgeTranscriptions_intermediate" ]; then
        ls -la data/LangAgeTranscriptions_intermediate/
    else
        echo "   No intermediate directory found"
    fi
    
else
    echo "âŒ Output file not created"
    echo "â±ï¸  Processing time: ${DURATION} seconds ($(($DURATION / 60)) minutes)"
fi

echo ""
echo "ğŸ‰ Complete dataset processing finished!"
echo ""
echo "ğŸ“ Final dataset location: /sc/home/hanno.mueller/TTS-LangAge/data/LangAgeTranscriptions.csv"
echo "ğŸ“ Intermediate files: /sc/home/hanno.mueller/TTS-LangAge/data/LangAgeTranscriptions_intermediate/"
echo ""
echo "Usage:"
echo "   ./scripts/process_complete_dataset.sh                    # Process with checkpoint-6000"
echo "   ./scripts/process_complete_dataset.sh checkpoint-8000    # Process with specific checkpoint"
